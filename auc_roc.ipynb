{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Area Under the Curve of the Receiver Operating Characteristic\n",
    "\n",
    "In this notebook we'll explore the notion of the receiver operating characteristic. Let's import some tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "First let's generate some data using sklearn's make_classification tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=10000, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "\n",
    "X_train_sc = ss.fit_transform(X_train)\n",
    "X_test_sc = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Now we'll fit a LogisticRegression object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(solver='liblinear')\n",
    "\n",
    "logreg.fit(X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions\n",
    "\n",
    "***`.predict()` vs. `.predict_proba()`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.predict_proba(X_test_sc)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.predict(X_test_sc)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, logreg.predict(X_test_sc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining true/false positives/negatives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp, tn, fp, fn = cm[1][1], cm[0][0], cm[0][1], cm[1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUC-ROC\n",
    "\n",
    "The Receiver Operating Characteristic curve plots the true-positive rate vs. the false-positive rate. Let's define these now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpr = tp / (tp + fn)\n",
    "fpr = fp / (fp + tn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thresholds\n",
    "\n",
    "Wait. How does this make sense? Doesn't a classifier just have a certain number of true positives, false positives, and all the rest? And so wouldn't a \"plot\" of these rates just be a single point on a graph?\n",
    "\n",
    "Consider a prediction for a particular data point. The features have particular values that lead the model to predict 0 or 1, one way or the other. But the model doesn't merely spit out 0's and 1's: As we just saw, there is a *calculation* done here. Let's look again at the predicted probabilities of class membership for a particular point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.predict_proba(X_test_sc)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the default behavior is simply to take the larger of these values as the \"real\" prediction. Since $0.84 > 0.16$, we'll understand the model to be predicting this point to belong to class \"1\" (or the positive class). An equivalent way of understanding the default behavior is that we:\n",
    "\n",
    "- round the predicted numbers up to 1 if they are at least as large as 0.5; and\n",
    "- round them down to 0 if they are less than 0.5.\n",
    "\n",
    "Since the probabilities must sum to 1, there will never be any problem with this algorithm.\n",
    "\n",
    "But we don't have to do things this way. Suppose we're building a model that predicts the presence of prostate cancer from X-ray scans of prostates. And suppose we get a pair of probabilities for some particular scan that look like this:\n",
    "\n",
    "- pred_neg: 0.52, pred_pos: 0.48\n",
    "\n",
    "Because false negatives (cancerous prostates mislabeled) are *much* more costly than false positives (non-cancerous prostates mislabeled), we may well want to **adjust our threshold** of classification. We might want to have our model predict \"positive\" if the corresponding probability is, say, as low as 0.4, or maybe even as low as 0.1. (Speaking for myself, if there was even a 10% chance that my prostate was cancerous, I think I'd probably want to know about it.)\n",
    "\n",
    "Clearly, the true- and false-positive rates will change if we make this adjustment to the threshold. In fact, in the present case that was the whole point of making the adjustment: We want to minimize our false negatives.\n",
    "\n",
    "So this is how the plot of these rates takes shape.\n",
    "\n",
    "Let's build a function that will take in our data, together with a threshold setting, and return the corresponding true- and false-positive rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_rates(X_train, X_test, y_train, y_test, thresh):\n",
    "    logreg = LogisticRegression(solver='liblinear')\n",
    "    logreg.fit(X_train, y_train)\n",
    "    y_hat_probs = logreg.predict_proba(X_test)\n",
    "    y_hat = []\n",
    "    for val in y_hat_probs:                                 # Each element in y_hat_probs is an array.\n",
    "        if val[0] < thresh:                                 # We'll set our own threshold for classifying\n",
    "            y_hat.append(1)                                 # a test point as positive! The lower my threshold,\n",
    "        else:                                               # the fewer predicted positives I'll have. For the\n",
    "            y_hat.append(0)                                 # cancer example, I'd want to set a *high* threshold.\n",
    "    cm = confusion_matrix(y_test, y_hat)\n",
    "    tp, tn, fp, fn = cm[1][1], cm[0][0], cm[0][1], cm[1][0]\n",
    "    tpr = tp / (tp + fn)\n",
    "    fpr = fp / (fp + tn)\n",
    "    return tpr, fpr, f'tpr:{round(tpr, 3)}, fpr:{round(fpr, 3)}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True- and false-positive rates for various thresholds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in np.linspace(0, 1, 11):\n",
    "    print(f'Rates at threshold = {round(x, 1)}: ' + classify_rates(X_train_sc, X_test_sc, y_train, y_test, x)[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As my threshold goes up, I'll have more positive predictions, which means I'll have both more true positives and more false positives.\n",
    "\n",
    "Note:\n",
    "\n",
    "- I can artificially increase my true-positive rate to 1 by setting my threshold to 1, but at that point my false-positive rate is also 1! I'll have no true negatives and no false negatives. This will arise naturally if my training data has **very few (actual) negatives**. This was the problem in Lottery Scenario 1.\n",
    "- I can artificially reduce my false-positive rate to 0 by setting my threshold to 0, but at  that point my true-positive rate is also 0! I'll have no true positives and no false positives. This will arise naturally if my training data has **very few (actual)  positives**. This was the problem in Lottery Scenario 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Area Under the Curve\n",
    "\n",
    "The ROC curve will be a plot of tpr (on the y-axis) vs. fpr (on the x-axis). There will always be a point at (0, 0) and another at (1, 1). The question is what happens in the middle. Since we want our y-values to be as high as possible for any particular x-value, a natural metric is to calculate the **area under the curve**. The larger the area, the better the classifier. The maximum possible area is the area of the whole box between 0 and 1 on both axes, so that's a **maximum area of 1**.\n",
    "\n",
    "What's the minimum? Well that depends on the ratios of (actual) positive and negatives in my data, in much the way that a baseline accuracy score does.\n",
    "\n",
    "Remember: If my test data comprises 90% positives and only 10% negatives, then a simply classifier that always predicts \"positive\" will be 90% accurate! And so that would be the baseline level for a classifier on that data.\n",
    "\n",
    "If we have equal numbers of positives and negatives, then we can set an **abolute minimum area of 0.5**. That's the \"curve\" we'd get by plotting a straight diagonal line from (0, 0) to (1, 1).\n",
    "\n",
    "Why? The area under the curve really represents the test's ability to **discriminate** positives from negatives. Suppose I randomly took several pairs of points, one positive and one negative, and checked my test's predictions. The area under the curve represents a threshold-independent measure of how often my test would get the two predictions correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the Curve\n",
    "\n",
    "Let's plot our own ROC curve. We'll create an array of different thresholds and use our `classify_rates()` function to get the true- and false-positive rates for each threshold.\n",
    "\n",
    "One way of choosing a threshold **independently of business concerns** is to select the point on the curve that is furthest from (1, 0), the \"worse-case\" point where our true-positive rate is 0 and our false-positive rate is 1. So let's find that point as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tprs = []\n",
    "fprs = []\n",
    "diffs = []\n",
    "for x in np.linspace(0, 1, 101):\n",
    "    fprs.append(classify_rates(X_train_sc, X_test_sc, y_train, y_test, x)[1])\n",
    "    tprs.append(classify_rates(X_train_sc, X_test_sc, y_train, y_test, x)[0])\n",
    "    xy2 = (fprs[-1] +  tprs[-1]) / 2\n",
    "    diffs.append(np.sqrt((xy2 - fprs[-1])**2 + (xy2 - tprs[-1])**2))\n",
    "    \n",
    "max_dist = diffs.index(np.max(diffs))\n",
    "print(f\"\"\"With a threshold of {(max_dist - 1) / 100}: \\n\"\"\"\n",
    "      f\"\"\"\\tYou\\'ll have a True Positive Rate of {round(tprs[max_dist], 3)} \\n\"\"\"\n",
    "      f\"\"\"\\tand a False Positive Rate of {round(fprs[max_dist], 3)}\"\"\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.plot(fprs[:max_dist], tprs[:max_dist], 'r.')\n",
    "ax.plot(fprs[max_dist], tprs[max_dist], 'ko', ms=10)\n",
    "ax.plot(fprs[max_dist + 1:], tprs[max_dist + 1:], 'r.')\n",
    "ax.plot(fprs, fprs, '.');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn's `roc_auc_score()` function will compute the area under the curve for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(roc_auc_score(y_test, logreg.predict(X_test_sc)), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare our curve with scikit-learn's:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, logreg.predict(X_test_sc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(fpr, tpr);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn only shows us the optimal threshold, but it appears to be very similar to ours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
